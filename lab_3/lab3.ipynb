{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Розширені алгоритми прогнозування за допомогою нейронних мереж на прикладах медичних даних**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мета роботи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ця лабораторна робота присвячена прогнозуванню динаміки поширення COVID-19 у світі за допомогою нейронних мереж різних структур. Мета роботи навчитисяч робити прогнози на основі лінійної регресії, зворотного розповсюдження та нейронних мереж довгої короткочасної пам’яті (long short-term memory LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Виконання"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сьогодні існує багато відкритих даних про поширення COVID-19 у світі. Однак представлено небагато інструментів для прогнозування цих процесів. Ця лабораторна робота покаже, як можна завантажувати дані з відкритих джерел, виконувати попередній аналіз даних, перетворювати та очищати дані, виконувати кореляційний та затримковий аналіз.\n",
    "\n",
    "Далі будуть розглянуті три різні математичні моделі для розрахунку прогнозування.\n",
    "\n",
    "Для цього буде продемонстровано поділ набору даних на навчальні та тестові вибірк. Буде продемонстровно як нормалізувати дані та провести попередній аналіз. Буде показано, як побудувати моделі та набори даних для використання двох різних нейронних мереж. Наступним кроком є побудова прогнозу та порівняння точності та адекватності отриманих моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матеріали та методи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У цій лабораторній роботі буде вивчено основні методи прогнозування часових рядів. Лабораторна робота складається з таких етапів:\n",
    "* Завантаження та попередній аналіз даних\n",
    "* Лінійна регресія\n",
    "* Метод зворотного поширення помилки (Back Propagation NN)\n",
    "* Довгої короткочасної пам’яті- LSTM\n",
    "\n",
    "На першому етапі буде показано, як завантажити дані та заздалегідь підготувати їх до аналізу:\n",
    "* завантажити дані\n",
    "* змінити типи даних стовпців\n",
    "* фільтрація рядків\n",
    "* усунення відсутніх даних\n",
    "* перетворення набору даних\n",
    "* нормалізація даних\n",
    "\n",
    "Під час наступних кроків буде продемонстрованмо три різні моделі прогнозування часових рядів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статистичні дані отримані з https://ourworldindata.org/coronavirus на основі Creative Commons BY license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Середовище виконання\n",
    "* [Python](https://www.python.org)\n",
    "* [Pandas](https://pandas.pydata.org)\n",
    "* Statistics\n",
    "* [NumPy](https://numpy.org)\n",
    "* [Matplotlib](https://matplotlib.org)\n",
    "* [Keras](https://keras.io)\n",
    "* [Scikit-Learn](https://scikit-learn.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отримані навики\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Після виконання даної лабораторної роботи отримуються навики:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Завантаження набору даних з файлів *.csv\n",
    "* Автоматичне змінення даних в наборі даних\n",
    "* Перетворення таблиць\n",
    "* Візуалізація даних за допомогою  pandas\n",
    "* Створення прогнозних моделей на основі нейронних мереж\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Індивідуальний варіант №16 (1)\n",
    "\n",
    "Реалізувати прогнозування за трьома моделями для групи країн з індивідуального завдання на основі даних https://ourworldindata.org/coronavirus.\n",
    "\n",
    "\n",
    "1. Реалізувати прогнозування поширення COVID-19 у групі скандинавських країн.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завантаження та попередній аналіз даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Завантаження даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед початком необхідно імпортувати додаткові бібліотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будемо використовувати той самий набір даних, що і в попередній лабораторній роботі.\n",
    "Тому повторюємо  попередню обробку даних , як це було раніше у лабораторній роботі 2.\n",
    "Наступним кроком є завантаження файлу даних із[open repository produced by Our World in Data under the Creative Commons BY license](https://ourworldindata.org/coronavirus)\n",
    " з допомогою **[read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'covid-data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_1560/340855548.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mcovid_word\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'covid-data.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mcovid_word\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\yupyter_for_iad\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\yupyter_for_iad\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 586\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    587\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\yupyter_for_iad\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    481\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 482\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    483\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    484\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\yupyter_for_iad\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    810\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 811\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    812\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    813\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\yupyter_for_iad\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1038\u001B[0m             )\n\u001B[0;32m   1039\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1040\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1041\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1042\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\yupyter_for_iad\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[1;31m# open handles\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\yupyter_for_iad\\venv\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m         \"\"\"\n\u001B[1;32m--> 222\u001B[1;33m         self.handles = get_handle(\n\u001B[0m\u001B[0;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    224\u001B[0m             \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\yupyter_for_iad\\venv\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    700\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"b\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    701\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 702\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'covid-data.csv'"
     ]
    }
   ],
   "source": [
    "covid_word = pd.read_csv('covid-data.csv')\n",
    "covid_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте вивчимо наш набір даних. Він складається з 86202 рядків × 59 стовпців.\n",
    "Перші 3 стовпці містять географічну інформацію. Графа 4 - дата вимірювання.\n",
    "Ще 55 - дані про COVID -19. Також у наборі даних спостерігаються деякі відсутні дані.\n",
    "Ми повинні бути впевнені, що Python правильно розпізнав типи даних. Для цього ми повинні використовувати **[pandas.info()]\n",
    "(https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html?highlight=info#pandas.DataFrame.info)**.**[pandas.info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html?highlight=info#pandas.DataFrame.info)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "covid_word.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бачимо, 54 стовпці даних COVID-19 були розпізнані правильно (float64).\n",
    "Перші 4 стовпці та одиниці тесту були визнані об’єктами. Давайте дослідимо їх:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fields = ['iso_code', 'continent', 'location', 'tests_units']\n",
    "covid_word[fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необхідно відобразити інформацію про поле дата."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "covid_word['date']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зміна типів стовпців"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the columns: 'iso_code', 'continent', 'location', 'tests_units' have many repetitions and should be assigned to categorical fieldsБачимо, що колонки: 'iso_code', 'continent', 'location', 'tests_units'мають багато повторів і повинні бути віднесені до категорійних полів\n",
    "**([pandas.astype()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html?highlight=astype#pandas.DataFrame.astype))**.\n",
    "Поле \"дата\" слід перетворити на тип DateTime   **([pandas.to_datetime()](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html))**. Для перегляду результатів можна використати наступний код  **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fields = ['iso_code', 'continent', 'location', 'tests_units']\n",
    "covid_word[fields] =covid_word[fields].astype('category')\n",
    "covid_word.loc[:, 'date'] = pd.to_datetime(covid_word['date'])\n",
    "covid_word[fields].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фільтрація рядків"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бачимо, що набір даних містить інформацію про 6 континентах та 219 країнах. Поле \"test_units\" складається з 4 категорій. Щоб показати список країн, треба використати**[pandas.Series.cat.categories](https://pandas.pydata.org/docs/reference/api/pandas.Series.cat.categories.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "covid_word['location'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дослідимо динаміку нових випадків COVID-19 для окремої країни.\n",
    "Будемо використовувати Україну для збереження моделей та подальшого прогнозу.\n",
    "Вибір країн здійснюємо згідно індивідуального завдання.\n",
    "Використовуємо для цього фільтр pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "covid_word.index = covid_word['date']\n",
    "##YOUR CODE GOES HERE##\n",
    "\n",
    "c_covid_Denmark = covid_word.loc[covid_word['location'] == \"Denmark\"]\n",
    "c_covid_Norway = covid_word.loc[covid_word['location'] == \"Norway\"]\n",
    "c_covid_Sweden = covid_word.loc[covid_word['location'] == \"Sweden\"]\n",
    "\n",
    "#All\n",
    "c_covid = covid_word.loc[(covid_word['location'] == \"Denmark\") |\n",
    " (covid_word['location'] == \"Norway\") | (covid_word['location'] == \"Sweden\")]\n",
    "\n",
    "c_covid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виберемо поля 'new_cases', 'new_cases_smoothed' для прогнозування. Перш за все, необхідно візуалізувати ці дані."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fields = ['new_cases', 'new_cases_smoothed']\n",
    "\n",
    "c_covid[fields].plot()\n",
    "plt.title(\"ALL COUNTRIES\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "c_covid_Denmark[fields].plot()\n",
    "plt.title(\"DENMARK\")\n",
    "plt.show()\n",
    "\n",
    "c_covid_Norway[fields].plot()\n",
    "plt.title(\"NORWAY\")\n",
    "plt.show()\n",
    "\n",
    "c_covid_Sweden[fields].plot()\n",
    "plt.title(\"SWEDEN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Видалення пропущених даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бачимо, у графіку нових випадків є великі коливання. Спробуємо скласти прогноз для цих хвиль. Перш за все, ми повинні видалити відсутні дані за допомогою [*pandas.DataFrame.dropna()*](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "c_covid = c_covid[fields].dropna()\n",
    "c_covid_Denmark = c_covid_Denmark[fields].dropna()\n",
    "c_covid_Norway = c_covid_Norway[fields].dropna()\n",
    "c_covid_Sweden = c_covid_Sweden[fields].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перетворення даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Якщо ми хочемо зробити прогноз часових рядів, єдине припущення, яке можна зробити - дані на сьогоднішній день залежать від значень попередніх днів. Щоб перевірити чи є залежності, необхідно провести кореляційний аналіз між ними. Для цього потрібно зробити:\n",
    "1. дублювання часового ряду даних та переміщення даних вертикально вниз протягом певної кількості днів затримки (lag)\n",
    "2. видалення відсутніх даних на початку та в кінці (вони формуються шляхом вертикального зсуву  (**[pandas.DataFrame.shift()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html)**)\n",
    "3. обчислення коефіцієнта кореляції між отриманими рядами.\n",
    "\n",
    "Оскільки цю операцію слід виконувати для різних значень затримки (lag), зручно створити окрему функцію:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lag_correlation_ts(y, x, lag):\n",
    "    \"\"\"\n",
    "    Lag correlation for 2 DateSeries\n",
    "    :param y: fixed\n",
    "    :param x: shifted\n",
    "    :param lag: lag for shifting\n",
    "    :return: DataFrame of lags correlation coefficients\n",
    "    \"\"\"\n",
    "    r = [0] * (lag + 1)\n",
    "    y = y.copy()\n",
    "    x = x.copy()\n",
    "    y.name = \"y\"\n",
    "    x.name = \"x\"\n",
    "\n",
    "    for i in range(0, lag + 1):\n",
    "        ds = y.copy().to_frame()\n",
    "        ds = ds.join(x.shift(i), how='outer')\n",
    "        r[i] = ds.corr().values[0][1]\n",
    "    r = pd.DataFrame(r)\n",
    "    r.index.names = ['Lag']\n",
    "    r.columns = ['Correlation']\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cтворимо цільовий (target) набір даних для кожної з країн та разом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_dataset_Denmark = c_covid_Denmark['new_cases']\n",
    "y_dataset_Norway = c_covid_Norway['new_cases']\n",
    "y_dataset_Sweden = c_covid_Sweden['new_cases']\n",
    "\n",
    "#All\n",
    "y_dataset = c_covid['new_cases']\n",
    "y_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестуємо 30-денну затримку для усіх трьох країн разом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "l = pd.DataFrame(lag_correlation_ts(y_dataset, y_dataset, 30)) #For time series we should use y_Dataset like input and output\n",
    "l_Norway = pd.DataFrame(lag_correlation_ts(y_dataset_Norway, y_dataset_Norway, 30)) #For time series we should use y_Dataset like input and output\n",
    "l_Sweden = pd.DataFrame(lag_correlation_ts(y_dataset_Sweden, y_dataset_Sweden, 30)) #For time series we should use y_Dataset like input and output\n",
    "l_Denmark = pd.DataFrame(lag_correlation_ts(y_dataset_Denmark, y_dataset_Denmark, 30)) #For time series we should use y_Dataset like input and output\n",
    "\n",
    "print(l)\n",
    "\n",
    "l.plot(title=\"All Lag\")\n",
    "l_Norway.plot(title=\"Norway Lag\")\n",
    "l_Denmark.plot(title=\"Denmark Lag\")\n",
    "l_Sweden.plot(title=\"Sweden Lag\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бачимо, що на ділянці затримки спостерігаються деякі хвилі. З таблиці видно, що на кожен сьомий день припадає пік. (Максимальний час затримки: 7, 14, 21 тощо). Це пов'язано з тижневим циклом.\n",
    "Будь-яку модель прогнозу можна показати у вигляді чорного ящика типу введення-цілі. Цільовими мають бути дані вихідного часового ряду, а вхідними - значення за попередні дні. (Див. рис.1)\n",
    "Щоб автоматизувати цей процес, давайте зробимо універсальну функцію перетворення часових рядів для створення цього набору даних."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/data-science-in-health-care-advanced-prognostication-using-by-neural-networks/FM.png\" width=\"1000\" alt=\"cognitiveclass.ai logo\"  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рис.1 Структура моделі прогнозування"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Щоб автоматизувати цей процес, треба зробити універсальну функцію перетворення часових рядів для створення цього набору даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(in_data, tar_data, n_in=1, dropnan=True, target_dep=False):\n",
    "    \"\"\"\n",
    "    Transformation into a training sample taking into account the lag\n",
    "     : param in_data: Input fields\n",
    "     : param tar_data: Output field (single)\n",
    "     : param n_in: Lag shift\n",
    "     : param dropnan: Do destroy empty lines\n",
    "     : param target_dep: Whether to take into account the lag of the input field. If taken into account, the input will start with a lag 1\n",
    "     : return: Training sample. The last field is the source\n",
    "    \"\"\"\n",
    "\n",
    "    n_vars = in_data.shape[1]\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    if target_dep:\n",
    "        i_start = 1\n",
    "    else:\n",
    "        i_start = 0\n",
    "    for i in range(i_start, n_in + 1):\n",
    "        cols.append(in_data.shift(i))\n",
    "        names += [('%s(t-%d)' % (in_data.columns[j], i)) for j in range(n_vars)]\n",
    "\n",
    "    if target_dep:\n",
    "        for i in range(n_in, -1, -1):\n",
    "            cols.append(tar_data.shift(i))\n",
    "            names += [('%s(t-%d)' % (tar_data.name, i))]\n",
    "    else:\n",
    "        # put it all together\n",
    "        cols.append(tar_data)\n",
    "        names.append(tar_data.name)\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як згадувалося вище, при прогнозуванні часових рядів поля введення та виведення є однаковими, лише зміщуються на затримку (lag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_Norway = series_to_supervised(pd.DataFrame(y_dataset_Norway), y_dataset_Norway, 14)\n",
    "dataset_Sweden = series_to_supervised(pd.DataFrame(y_dataset_Sweden), y_dataset_Sweden, 14)\n",
    "dataset_Denmark = series_to_supervised(pd.DataFrame(y_dataset_Denmark), y_dataset_Denmark, 14)\n",
    "\n",
    "dataset = series_to_supervised(pd.DataFrame(y_dataset), y_dataset, 14)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отже перший і останній стовпці містять однакові цільові дані. Тепер треба створити набори даних вхідних (**X**) та вихідних (**Y**) даних для моделей прогнозування."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "col = dataset.columns\n",
    "\n",
    "X_Norway, Y_Norway = dataset_Norway[col[1:-1]], dataset_Norway[col[-1]]\n",
    "X_Denmark, Y_Denmark = dataset_Denmark[col[1:-1]], dataset_Denmark[col[-1]]\n",
    "X_Sweden, Y_Sweden = dataset_Sweden[col[1:-1]], dataset_Sweden[col[-1]]\n",
    "\n",
    "X, Y = dataset[col[1:-1]], dataset[col[-1]]\n",
    "print(\"Input: \", X.columns)\n",
    "print(\"Target:\", Y.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Нормування даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Після цього ми повинні нормалізувати всі дані. Для цього слід використовувати модуль  [**sklearn.preprocessing.MinMaxScaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n",
    "Це дозволяє нам нормалізувати   [**fit_transform()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.fit_transform) легко конвертувати всі дані назад: [**fit_transform()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_x_Sweden = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y_Sweden = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaler_x_Denmark = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y_Denmark = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaler_x_Norway = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y_Norway = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_x_Sweden = scaler_x_Sweden.fit_transform(X_Sweden)\n",
    "scaled_y_Sweden = scaler_y_Sweden.fit_transform(Y_Sweden.values.reshape(-1, 1))\n",
    "\n",
    "scaled_x_Denmark = scaler_x_Denmark.fit_transform(X_Denmark)\n",
    "scaled_y_Denmark = scaler_y_Denmark.fit_transform(Y_Denmark.values.reshape(-1, 1))\n",
    "\n",
    "scaled_x_Norway = scaler_x_Norway.fit_transform(X_Norway)\n",
    "scaled_y_Norway = scaler_y_Norway.fit_transform(Y_Norway.values.reshape(-1, 1))\n",
    "\n",
    "#ALL\n",
    "scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_x = scaler_x.fit_transform(X)\n",
    "scaled_y = scaler_y.fit_transform(Y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we are going to form training and test DataSets using [**sklearn.model_selection.train_test_split()**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). We will make them at the ratio of 70/30. Without shuffling. It means, that test samples are located in the end of **X** and **Y** DataSets.\n",
    "\n",
    "Input normalized DataSets: **X_train, X_test**\n",
    "\n",
    "Target normalized DataSets: **y_train, y_test**\n",
    "\n",
    "Після цього ми збираємося сформувати навчальні та перевірити набори даних за допомогою [**sklearn.model_selection.train_test_split()**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "Ми зробимо їх у співвідношенні 70/30. Без перемішування. Це означає, що тестові зразки розташовані в кінці наборів даних X і Y.\n",
    "\n",
    "Введення нормалізованих наборів даних: **X_train, X_test**\n",
    "\n",
    "Цільові нормалізовані набори даних: **y_train, y_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_Sweden, X_test_Sweden, y_train_Sweden, y_test_Sweden = train_test_split(scaled_x_Sweden, scaled_y_Sweden, test_size=0.3, shuffle=False)\n",
    "X_train_Norway, X_test_Norway, y_train_Norway, y_test_Norway = train_test_split(scaled_x_Norway, scaled_y_Norway, test_size=0.3, shuffle=False)\n",
    "X_train_Denmark, X_test_Denmark, y_train_Denmark, y_test_Denmark = train_test_split(scaled_x_Denmark, scaled_y_Denmark, test_size=0.3, shuffle=False)\n",
    "\n",
    "#ALL\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_x, scaled_y, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зараз усі дані нормалізовані. Однак, для порівняння з результатами, нам потрібні дані реального масштабу навчального та тестового набору даних::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_train_Sweden = scaler_y_Sweden.inverse_transform(y_train_Sweden).flatten()\n",
    "res_test_Sweden = scaler_y_Sweden.inverse_transform(y_test_Sweden).flatten()\n",
    "\n",
    "res_train_Norway = scaler_y_Norway.inverse_transform(y_train_Norway).flatten()\n",
    "res_test_Norway = scaler_y_Norway.inverse_transform(y_test_Norway).flatten()\n",
    "\n",
    "res_train_Denmark = scaler_y_Denmark.inverse_transform(y_train_Denmark).flatten()\n",
    "res_test_Denmark = scaler_y_Denmark.inverse_transform(y_test_Denmark).flatten()\n",
    "\n",
    "#ALL\n",
    "res_train = scaler_y.inverse_transform(y_train).flatten()\n",
    "res_test = scaler_y.inverse_transform(y_test).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цільові набори даних реального масштабу: **res_train, res_test**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лінійна регресія"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перш за все, треба створити модель. Ми перевіримо три типи моделей. Лінійна регресія, багатошарова нейронна мережа зі зворотним поширенням помилки та нейромережа з довгою короткостроковою пам'яттю. Створимо a [**LinearRegression()**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Після цього наша модель повинна бути тренована на навчальному наборі даних. Незалежно від типу моделі, для цього використовується функція fit ()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоді ми можемо перевірити його на тестовому наборі даних та використовувати для прогнозування."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test_ln = regressor.predict(X_test)\n",
    "y_pred_test_ln = scaler_y.inverse_transform(y_pred_test_ln).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналізуємо точність моделі, використовуючи **[sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Correlation train\", regressor.score(X_train, y_train))\n",
    "print(\"Correlation test\", regressor.score(X_test, y_test))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_test_ln))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_test_ln))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_ln)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод зворотного поширення помилки (Back Propagation NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сучасний підхід до встановлення складних функціональних залежностей полягає у використанні нейронних мереж. Класична нейронна мережа - це багатошарова нейронна мережа із зворотним поширенням. [**multilayer neural network with back propagation**](https://en.wikipedia.org/wiki/Backpropagation).\n",
    "\n",
    "Для цього використаємо [**keras**](https://keras.io) фреймворк.\n",
    "Перш за все, ми повинні створити модель нейронної мережі як окрему функцію.\n",
    "\n",
    "Нейронна мережа - це послідовність шарів. Функція Sequential ()  [**Sequential()**](https://keras.io/guides/sequential_model/) використовується для створення мережі.\n",
    "\n",
    "Вихідний шар буде складатися з одного нейрона, оскільки у нас на виході є лише одне значення. [**keras.layers.Dense()**](https://keras.io/api/layers/core_layers/dense/).\n",
    "\n",
    "Щоб уникнути проблем з перенавчанням, ми будемо використовувати додаткові шари[**keras.layers.Dropout()**](https://keras.io/api/layers/regularization_layers/dropout/).\n",
    "\n",
    "Модель необхідно зкомпілювати для підгонки та прогнозування:\n",
    " [**keras.Model.compile()**](https://keras.io/api/models/model_training_apis/).\n",
    "Сучасний підхід до визначення складних функціональних залежностей полягає у використанні нейронних мереж. Класична нейронна мережа - це багатошарова нейронна мережа із зворотним поширенням.\n",
    "\n",
    "Для цього ми будемо використовувати фреймворк keras. Перш за все, ми повинні створити модель нейронної мережі як окрему функцію.\n",
    "\n",
    "Нейронна мережа - це послідовність шарів. Функція Sequential () використовується для створення мережі.\n",
    "\n",
    "Створимо мережу, яка складається з 2 прихованих шарів. Кожен з яких складається з 100 нейронів. keras.layers.Dense ().\n",
    "\n",
    "Щоб уникнути проблем з перепідготовкою, ми будемо використовувати додаткові шари keras.layers.Dropout ().\n",
    "\n",
    "Вихідний шар буде складатися з одного нейрона, оскільки у нас на виході є лише одне значення.\n",
    "\n",
    "Модель слід зібрати для підгонки та прогнозування: keras.Model.compile ()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def BP_model(X):\n",
    "    \"\"\"\n",
    "    Multilayer neural network with back propagation.\n",
    "    :param X: Input DataSet\n",
    "    :return: keras NN model\n",
    "    \"\"\"\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Після того, як функція моделі буде побудована, необхідно безпосередньо створити нейронну мережу та вказати параметри навчання:   [**keras.wrappers.scikit_learn.KerasRegressor()**](https://keras.io/zh/scikit-learn-api/). Також слід визначити кількість епох припасування та розмір навчальної вибірки: [**epoch and batch size**](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "epochs = 1000\n",
    "\n",
    "batch_size=int(y_train.shape[0]*.1)\n",
    "\n",
    "estimator = KerasRegressor(build_fn=BP_model, X=X_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s train our model for  epochs.\n",
    "It should be noted that the fitting process is very slow. Therefore, we saved our fitted model to a file.\n",
    "To save time, we will upload the fitted model.\n",
    "If you like, you can leave the parameter  to refit the model.\n",
    "If you like, you can leave the parameter  to resave the model.Тепер давайте навчимо нашу модель **1000** епох. Слід зазначити, що процес підгонки відбувається дуже повільно. Тому труба зберегли таку модель у файл. Щоб заощадити час,  завантажимо підігнану модель. Якщо потрібно,  можна залишити параметр **fitting on True** рівним True, щоб оновити модель. Якщо потрібно,  можна залишити параметр **fitting_save on True**, щоб зберегти модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fitting = True\n",
    "fitting_save = True\n",
    "\n",
    "import pickle\n",
    "\n",
    "if fitting:\n",
    "    history=estimator.fit(X_train,y_train, validation_data=(X_test,y_test)) # Fitting model\n",
    "    if fitting_save:\n",
    "        # Save model\n",
    "        estimator.model.save('BP_saved_model.h5')\n",
    "        print(\"Saved model to disk\")\n",
    "        with open('history.pickle', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "# load model\n",
    "from keras.models import load_model\n",
    "\n",
    "# Instantiate the model as you please (we are not going to use this)\n",
    "estimator = KerasRegressor(build_fn=BP_model, X=X_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "# This is where you load the actual saved model into a new variable.\n",
    "estimator.model = load_model('BP_saved_model.h5')\n",
    "with open('history.pickle', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажемо динаміку втрат та валідації [**loss and validation loss dynamics**](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history['loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='test')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.plot(history.history['acc'], label='acc')\n",
    "#plt.plot(history.history['val_acc'], label='acc test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отже нейромережа добре підігнана, і не спостерігається її надмірної адаптації. Давайте обчислимо прогноз навчальної (**res_train_ANN**) і тестової  (**res_test_ANN**) вибірки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Давайте обчислимо прогноз і зробимо зворотну нормалізацію до реального масштабу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_tr=estimator.predict(X_train)\n",
    "res_ts=estimator.predict(X_test)\n",
    "res_train_ANN=scaler_y.inverse_transform(res_tr.reshape(-1, 1)).flatten()\n",
    "res_test_ANN=scaler_y.inverse_transform(res_ts.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте порівняємо точність лінійної регресії та нейронної мережі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Correlation train\", np.corrcoef(res_train, res_train_ANN)[0,1])\n",
    "print(\"Correlation train\", np.corrcoef(res_test, res_test_ANN)[0,1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, res_test_ANN))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, res_test_ANN))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, res_test_ANN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Можна бачити, що отримано дещо кращі результати для нейронної мережі, ніж для лінійної регресії."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод довгої короткочасної пам’яті Long Short-Term Memory - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " На відміну від стандартних нейронних мереж, що передають інформацію, [** LSTM **] (https://en.wikipedia.org/wiki/Long_short-term_memory) має зв'язки зворотного зв'язку. Він може обробляти не тільки окремі точки даних, а й цілі послідовності даних (наприклад, мовлення, відео чи часові ряди).\n",
    "\n",
    "У разі часових рядів нейронна мережа має один вхід і один вихід. Однак на вході повинен бути вектор значень часових рядів за попередній період часу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/data-science-in-health-care-advanced-prognostication-using-by-neural-networks/LSTM.png\" width=\"1000\" alt=\"cognitiveclass.ai logo\"  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для цього нам слід перетворити вхідні набори даних у 3D -форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x_LSTM = X_train.reshape((X_train.shape[0], 1, 14))\n",
    "test_x_LSTM = X_test.reshape((X_test.shape[0], 1, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте створимо нейронну мережу [**LSTM**](https://keras.io/api/layers/recurrent_layers/lstm/) що складається з одного шару LSTM та одного шару BP, як у попередньому випадку.\n",
    "As you can see, in this case our NN will consist of 7 LSTM and 7 BP neurons only. LSTM, що складається з одного шару LSTM та одного шару BP, як у попередньому випадку. Як бачите, у цьому випадку наша NN складатиметься лише з 7 нейронів LSTM і 7 нейронів шару BP ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "batch_size=int(y_train.shape[0]*.1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(7, input_shape=(train_x_LSTM.shape[1], train_x_LSTM.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(7, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1])) #activation='sigmoid'\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "#model.compile(loss='mae', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усі наступні етапи навчання, зберігання, читання та прогнозування подібні до тих, які ми використовували з попередньою нейронною мережею. Можна бачити, що всього 400 епох достатньо для LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fitting = True\n",
    "fitting_save = True\n",
    "epochs = 400\n",
    "\n",
    "if fitting:\n",
    "    history = model.fit(train_x_LSTM, y_train, epochs=epochs, batch_size=batch_size, validation_data=(test_x_LSTM, y_test), verbose=1, shuffle=False)\n",
    "    if fitting_save:\n",
    "    # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open(\"LSTM_model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"LSTM_model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        with open('history_LSTM.pickle', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "# load model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('LSTM_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"LSTM_model.h5\")\n",
    "with open('history_LSTM.pickle', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Давайте побудуємо динаміку втрат та втрат значень, як у попередньому випадку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##\n",
    "# plot history\n",
    "plt.figure()\n",
    "plt.plot(history['loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='test')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate our forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "res_tr_LSTM = model.predict(train_x_LSTM)\n",
    "res_ts_LSTM = model.predict(test_x_LSTM)\n",
    "res_train_LSTM=scaler_y.inverse_transform(res_tr_LSTM).flatten()\n",
    "res_test_LSTM=scaler_y.inverse_transform(res_ts_LSTM).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Correlation train\", np.corrcoef(res_train, res_train_LSTM)[0,1])\n",
    "print(\"Correlation train\", np.corrcoef(res_test, res_test_LSTM)[0,1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, res_test_LSTM))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, res_test_LSTM))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, res_test_LSTM)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the forecast results of the test data set are much better than ones of the previous models. Let's visualize these 3 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_pred_test_ln = pd.Series(y_pred_test_ln, name = 'Predicted test Linear Model')\n",
    "res_pred_test_ANN = pd.Series(res_test_ANN, name = 'Predicted test ANN')\n",
    "res_pred_test_LSTM = pd.Series(res_test_LSTM, name = 'Predicted test LSTM')\n",
    "\n",
    "df_2 = pd.DataFrame({'Actual test': res_test, 'Linear Model': res_pred_test_ln, 'ANN Model': res_pred_test_ANN,  'LSTM Model': res_pred_test_LSTM,})\n",
    "df_2.index = dataset.index[len(dataset)-len(res_test):]\n",
    "df_2.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бачимо,що модель LSTM дає ідеальний прогноз. Лінійна регресія - найшвидша модель прогнозування."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Висновки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "У цій лабораторній роботі розглянуто три типи моделей прогнозування. Реалізовано перетворення наборів даних для моделей введення-виведення. Здійснено нормалізація та обернена нормалізація для даних реального масштабу. Крім того,  продемонстровано, як розділити набори даних на навчальні та тестові. Показано, як побудувати моделі прогнозування часових рядів, використовуючи лагові перетворення. Розглянуто як встановлювати, зберігати та завантажувати різні типи нейронних мереж."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}